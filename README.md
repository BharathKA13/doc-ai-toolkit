# Document Portal - AI-Powered Document Intelligence Platform

> An intelligent document processing and chat system built with **LangChain**, **FAISS**, and **LLMs** for conversational document analysis, comparison, and question-answering.

## ğŸ¯ Overview

**Document Portal** is a FastAPI-based backend service that enables intelligent document processing through three core capabilities:

1. **Document Analysis** â€“ Extract structured metadata, summaries, and insights from documents
2. **Document Comparison** â€“ Semantically compare two documents to identify differences and similarities
3. **Conversational Chat** â€“ Ask questions about documents with context-aware responses using RAG

The system is production-ready with proper error handling, logging, session management, and Docker deployment support.

---

## âœ¨ Key Features

- **ğŸ“¤ Multi-format Document Support** â€“ PDF, DOCX, TXT files
- **ğŸ” RAG-Based Retrieval** â€“ FAISS vector store with semantic search
- **ğŸ’¬ Conversational Memory** â€“ Maintains chat history and context across turns
- **ğŸ“Š Document Analysis** â€“ Automatic extraction of metadata and summaries
- **ğŸ”€ Document Comparison** â€“ LLM-powered semantic document diffing
- **ğŸ“ Session Management** â€“ Persistent storage of chat sessions and indices
- **ğŸ› ï¸ Modular Architecture** â€“ Clean separation of concerns for easy extension
- **ğŸ“‹ Structured Logging** â€“ StructLog integration for production-grade monitoring
- **ğŸ³ Docker Ready** â€“ Dockerfile included for containerized deployment
- **âœ… Test Coverage** â€“ Pytest integration for quality assurance

---

## ğŸ—ï¸ Architecture

### System Design

```
User Interface (HTML/Templates)
         â†“
   FastAPI Backend
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â†“                     â†“                  â†“                    â†“
/analyze         /compare            /chat/index          /chat/query
DocumentAnalyzer  DocumentComparator  ChatIngestor       ConversationalRAG
  â†“                â†“                  â†“                    â†“
LLM + Prompts    LLM + Prompts      RecursiveCharacter   FAISS Retriever
                                    TextSplitter + LLM    + LLM

                    Model Loader (Groq/Google)
                     Embedding Model (Google)
```

### Core Components

| Component | Location | Purpose |
|-----------|----------|---------|
| **FastAPI Server** | `api/main.py` | REST API endpoints and UI serving |
| **Document Ingestion** | `src/document_ingestion/data_ingestion.py` | File parsing, chunking, FAISS indexing |
| **Document Analyzer** | `src/document_analyzer/data_analysis.py` | Metadata extraction and summarization |
| **Document Comparator** | `src/document_compare/document_comparator.py` | Semantic document comparison |
| **Conversational RAG** | `src/document_chat/retrieval.py` | Chat with documents using LCEL chains |
| **Model Loader** | `utils/model_loader.py` | LLM and embedding model initialization |
| **Logger** | `logger/custom_logger.py` | StructLog-based logging service |
| **Exception Handler** | `exception/custom_exception.py` | Custom error definitions |

---

## ğŸ”§ Tech Stack

### Core Dependencies
- **FastAPI** `0.116.1` â€“ Async web framework
- **LangChain** `0.3.27` â€“ LLM orchestration and RAG framework
- **FAISS** `1.11.0` â€“ Vector similarity search
- **LLM Providers:**
  - **Groq** â€“ `langchain-groq` (DeepSeek-R1 Distill LLaMA 70B)
  - **Google** â€“ `langchain-google-genai` (Gemini 2.5 Flash)
- **Embeddings** â€“ Google Text Embedding 004
- **Document Processing:**
  - **PyMuPDF** `1.26.3` â€“ PDF handling
  - **python-docx** `0.9` â€“ DOCX support
- **Server** â€“ Uvicorn `0.35.0`
- **Logging** â€“ StructLog `25.4.0`
- **UI** â€“ Jinja2 templates + static assets

### Development Tools
- **Testing** â€“ Pytest `8.4.1`
- **Environment** â€“ Python-dotenv `1.1.1`

---

## ğŸ“ Project Structure

```
doc-ai-toolkit/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app with 6 endpoints
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ document_ingestion/    # File parsing & FAISS indexing
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_analyzer/     # Document analysis & metadata extraction
â”‚   â”‚   â”œâ”€â”€ data_analysis.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_compare/      # Semantic document comparison
â”‚   â”‚   â”œâ”€â”€ document_comparator.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ document_chat/         # RAG-based conversational chat
â”‚       â”œâ”€â”€ retrieval.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ model_loader.py        # LLM & embedding model initialization
â”‚   â”œâ”€â”€ config_loader.py       # YAML configuration loader
â”‚   â”œâ”€â”€ document_ops.py        # Document processing utilities
â”‚   â”œâ”€â”€ file_io.py             # File I/O and session handling
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ logger/
â”‚   â”œâ”€â”€ custom_logger.py       # StructLog logger setup
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ exception/
â”‚   â”œâ”€â”€ custom_exception.py    # Custom exception definitions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ prompt/
â”‚   â””â”€â”€ prompt_library.py      # Prompt templates registry
â”œâ”€â”€ model/
â”‚   â””â”€â”€ models.py              # Pydantic models for structured output
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml            # LLM, embedding, and retriever config
â”œâ”€â”€ static/                    # CSS, JavaScript assets
â”œâ”€â”€ templates/                 # HTML templates
â”œâ”€â”€ tests/                     # Test files
â”œâ”€â”€ data/                      # Upload directory for documents
â”œâ”€â”€ faiss_index/               # FAISS vector indices
â”œâ”€â”€ logs/                      # Application logs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md
```

---

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.10+
- pip or conda

### 1. Clone Repository
```bash
git clone <repository-url>
cd doc-ai-toolkit
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Configure Environment
Create a `.env` file in the project root with your API keys:
```env
# LLM Provider - Groq
GROQ_API_KEY=your_groq_api_key

# Embeddings & LLM Provider - Google
GOOGLE_API_KEY=your_google_api_key

# Directories
FAISS_BASE=faiss_index
UPLOAD_BASE=data
FAISS_INDEX_NAME=index
```

### 5. Configure Models (Optional)
Edit `config/config.yaml` to select LLM providers and models:
```yaml
llm:
  groq:
    provider: "groq"
    model_name: "deepseek-r1-distill-llama-70b"
    temperature: 0
    max_output_tokens: 2048

embedding_model:
  provider: "google"
  model_name: "models/text-embedding-004"

retriever:
  top_k: 10
```

---

## ğŸ“– API Endpoints

### 1. Health Check
```
GET /health
```
Returns service status.

**Response:**
```json
{"status": "ok", "service": "document-portal"}
```

---

### 2. Analyze Document
```
POST /analyze
Content-Type: multipart/form-data

file: <PDF/DOCX/TXT file>
```

Extracts metadata, key topics, and summary from document.

**Response:**
```json
{
  "metadata": {
    "title": "...",
    "author": "...",
    "num_pages": 10,
    ...
  },
  "summary": "...",
  "key_topics": ["topic1", "topic2", ...]
}
```

---

### 3. Compare Documents
```
POST /compare
Content-Type: multipart/form-data

reference: <reference document>
actual: <document to compare>
```

Identifies differences, similarities, and changes between two documents.

**Response:**
```json
{
  "rows": [
    {"section": "Introduction", "difference": "..."},
    ...
  ],
  "session_id": "session_20250228_123456_abc123"
}
```

---

### 4. Build Chat Index
```
POST /chat/index
Content-Type: multipart/form-data

files: <list of documents>
session_id: <optional; auto-generated if not provided>
use_session_dirs: true
chunk_size: 1000
chunk_overlap: 200
k: 5
```

Ingests documents, chunks them, creates embeddings, and builds FAISS index for chat.

**Response:**
```json
{
  "session_id": "session_20250228_123456_abc123",
  "documents_ingested": 3,
  "chunks_created": 45,
  "embedding_model": "models/text-embedding-004",
  "retriever_k": 5
}
```

---

### 5. Chat Query
```
POST /chat/query
Content-Type: application/json

{
  "session_id": "session_20250228_123456_abc123",
  "question": "What is the main topic?",
  "chat_history": [
    {"role": "user", "content": "Previous question"},
    {"role": "assistant", "content": "Previous answer"}
  ]
}
```

Queries documents using RAG and returns context-aware answers.

**Response:**
```json
{
  "answer": "...",
  "source_chunks": [
    {"content": "...", "document": "filename.pdf", "page": 1}
  ],
  "session_id": "session_20250228_123456_abc123"
}
```

---

### 6. Serve UI
```
GET /
```

Serves HTML UI (`templates/index.html`).

---

## ğŸ’» Running the Application

### Local Development
```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

Access at: `http://localhost:8000`

### Docker Deployment
```bash
docker build -t document-portal:latest .
docker run -p 8080:8080 --env-file .env document-portal:latest
```

Access at: `http://localhost:8080`

---

## ğŸ§ª Testing

Run tests with pytest:
```bash
pytest tests/ -v
```

Run with coverage:
```bash
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ“ Usage Examples

### Example 1: Analyze a Document

```bash
curl -X POST "http://localhost:8000/analyze" \
  -F "file=@document.pdf"
```

### Example 2: Build Index and Chat

```bash
# Step 1: Build index
curl -X POST "http://localhost:8000/chat/index" \
  -F "files=@document1.pdf" \
  -F "files=@document2.pdf" \
  -F "session_id=my_session" \
  -F "chunk_size=1000" \
  -F "chunk_overlap=200" \
  -F "k=5"

# Step 2: Ask a question
curl -X POST "http://localhost:8000/chat/query" \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "my_session",
    "question": "What are the key findings?",
    "chat_history": []
  }'
```

---

## ğŸ” Configuration

### LLM Models Available

**Groq:**
- `deepseek-r1-distill-llama-70b` (default)

**Google:**
- `gemini-2.5-flash`

### Embedding Model
- `google/text-embedding-004`

### Retriever Settings
- **top_k** (default: 10) â€“ Number of chunks to retrieve
- **chunk_size** (default: 1000) â€“ Characters per chunk
- **chunk_overlap** (default: 200) â€“ Overlap between chunks

---

## ğŸ“Š Session Management

Sessions are automatically created with unique IDs:
```
session_YYYYMMDD_HHMMSS_hash
```

Each session stores:
- **FAISS Index** â€“ `faiss_index/session_*/`
- **Upload Data** â€“ `data/session_*/`
- **Metadata** â€“ Ingested file info and chunk metadata

---

## ğŸ“‹ Logging

Logs are written to `logs/` directory using StructLog. Key log levels:
- **INFO** â€“ API requests, session creation
- **WARNING** â€“ Retrieval fallbacks, edge cases
- **ERROR** â€“ Failed ingestion, LLM errors
- **DEBUG** â€“ Detailed processing steps (dev mode)

Access logs:
```bash
tail -f logs/app.log
```

---

## âš ï¸ Error Handling

Custom exceptions in `exception/custom_exception.py`:
- `DocumentPortalException` â€“ Application-level errors
- `DocumentIngestionError` â€“ File parsing/indexing failures
- `RetrievalError` â€“ FAISS or retrieval failures
- `LLMError` â€“ Model or API call failures

All errors are logged with full traceback.

---

## ğŸš€ Roadmap

- [ ] Multi-language document support
- [ ] Web UI improvements (Streamlit/Gradio)
- [ ] Hybrid search (BM25 + semantic)
- [ ] Source citation highlighting
- [ ] User authentication & role-based access
- [ ] Document versioning and audit logs
- [ ] Feedback loop for model refinement
- [ ] Batch document processing

---

## ğŸ“„ License

Developed by Bharath

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ” Troubleshooting

### Issue: "FAISS index not found"
**Solution:** Ensure documents are indexed via `/chat/index` endpoint before querying.

### Issue: "LLM API key not set"
**Solution:** Check `.env` file has valid `GROQ_API_KEY` and `GOOGLE_API_KEY`.

### Issue: "Unsupported file format"
**Solution:** Supported formats are `.pdf`, `.docx`, `.txt`. Convert other formats first.

### Issue: "Out of memory during indexing"
**Solution:** Reduce `chunk_size` or process fewer documents at once.

---

## ğŸ“Š Performance Notes

- **Embedding Generation:** ~1-2s per 1000 tokens (Google API)
- **FAISS Indexing:** ~0.1s per 1000 chunks (CPU)
- **Query Retrieval:** ~0.05s for top-k (FAISS)
- **LLM Response:** 2-10s depending on model and query complexity

**Recommendation:** For production use, consider deploying on a GPU instance for faster embedding generation.
